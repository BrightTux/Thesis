%!TEX ROOT = thesis.tex

\chapter{Conclusion}
\label{section:conclusion}
%\section*{Introduction}

Given that the use of video data as a means of surveillance is a common solution in the our society's yearning for safety and security, there are an abundance of such data sources. However, most of these data are stored unprocessed, and they are only retrieved when an incident dictates the need of doing so. This reactive approach towards an unforeseen event could proved costly to valuable resources such as manpower, time and finance of organisations. The goal of this thesis is to present a preemptive solution towards such incidences by converting large video footages into compact, meaningful semantics which can be retrieved on-the-fly. 

While there has been plenty of works done in the video surveillance domain, only a handful of these works revolve around a car park scenario. Among the existing literature that focused on car park scenarios, a majority of them tend to only focus on detecting the availability of parking spots.
Chapter \ref{section:litreview} provides an overview and critique of recent approaches to extract object semantics such as vehicle colour and motion together with relevant works involving retrieval engines. On a whole, the chapter suggests a considerable room for improvement and research in the field of computer vision and video retrieval for car park surveillance data.
Specifically, this thesis addresses two main objectives: (O1) to identify and extract suitable object semantics that can be easily interpretable while accurately describing scenarios in the car park, and (O2) to retrieve video footages using a video retrieval engine that is capable of taking in intuitive user-described queries. 

%In view of the advantages offered by segmentation of video data, 
The framework used in this work proposed the idea of segmenting input video data into a spatio-temporal cube (\textit{atoms}) structure, similar to that inspired by \cite{castanon2016retrieval}. Different from \cite{castanon2016retrieval}, these \textit{atoms} are used to describe specific locations in a 3D (XYT) space where the extracted vehicle colours and motions are represented. %occurred in both the \versionOneExt and \versionTwoExt modules. 
Besides, this segmentation process also allowed end users to describe trajectories in the form of drawings, which can be intuitively entered into the search canvas. 
The proposed method presented a robust retrieval engine that is able to return a wide range of ranked results without the need for meticulously drawn input trajectory by utilising similarity scores. 
% Methods introduced in both phases: \versionOneRet and \versionTwoRet presented obvious advantages and disadvantages: While the \versionOneRet method was able to accurately pinpoint locations in which an extracted semantic was found, the overall recall rate was significantly lower as the user had to provide very exact queries to retrieve a desired shot. This high accuracy \& low recall rate is useful when the exact query needs to be matched 100\%. 

Overall, the proposed framework and algorithms have been reasonably successful in extracting and retrieving object specific semantics. Given that the trajectory information was stored in a pseudo-Cartesian coordinate manner, the concept used in the proposed method can be extended to various datasets of similar properties. The concluding remarks and future extensions for each of the proposed methods are covered in the following sub-sections.

%\section*{Semantic Extraction, Representation and Retrieval}
\vspace{1em}
\subsection*{Colour Semantic Attribute Extraction}
The proposed vehicle colour extraction methods described in Section \ref{section:colourretrieval}  demonstrated robustness over existing handcrafted vehicle colour extraction methods in a low resolution scenario.
By leveraging on the extracted dominant colour of each vehicle over its entire tracked life cycle, the proposed method averaged out the extracted dominant colours to obtain a good estimate of the vehicle's colour.
As such, discoloration due to occlusion and shadows was averaged out as well in the proposed method. Experiments recorded in Chapter \ref{subsec:vehiclecolourchamferdistanceexperiment} showed that the proposed method was able to retrieve vehicle colours accurately: 91\% of the time within the top 3 results.   %While the proposed method might not be able to outperform techniques that apply CNN and SVMs such as those of \cite{hu2015vehicle} (94\%), the proposed method was able to put up a good fight. 
Although the definition of colour terms using the 330 Munsell Colour chips did not favour the proposed method, the adoption of~\citeA{riemersma}'s low cost estimation of difference in colours %in junction with the definition of colours terms 
along with the tuples provided by~\citeA{munroe2010color} proved to be fairly accurate when applied in a retrieval engine setting.     

In the future, the mapping of colour terms can be further improved by clustering the compressed data provided by~\citeA{zaslavsky2018efficient}. Besides, the segmented ROI from the response map of a CNN can also be applied for the extraction of colour information. The experiments in this thesis showed that the use of distance measures to differentiate colours may not be as efficient and accurate when compared to certain deep learning methods. Hence, deep learning models can be explored in the future to improve the classification of these colour terms.

\vspace{1em}
\subsection*{Motion Semantic Attribute Representation and Retrieval}
%As dictated in the scope of this work, the bounding box of the vehicles were assumed to be provided. Using the positions of the bounding boxes, the centroid of each box was used to mark the location of the vehicles. 
% In Chapter \ref{subsec:motions9binextract}, the motion information was derived using the previous and current positions of the vehicle. These motion information are then segmented into 9 bins representing various directional categories. While this method provided adequate representation of motion, the hard-coded motion information that was stored beforehand made the retrieval process challenging. Users had to provide the queries accurately to retrieve the desired video footage. At best, the \versionOneRet engine can achieve an accuracy of 95\%, however, at the cost of having to craft input queries very carefully. Also, this proposed method did not show promising recall rates. 

% The drawbacks in \versionOneRet led to the development of \versionTwoRet.
In Chapter \ref{section:motionextraction}, the proposed method suggested that the motion information can be left unprocessed. Instead, the individual location information obtained from the bounding box was represented using spatio-temporal cubes (or \textit{atoms}) can be stored in the database for the retrieval purpose. With this setup, Chamfer Distance was re-purposed as a means to measure the difference between the user's input against the stored data. The retrieval engine presented users with an input canvas where the motion of a desired vehicle could be drawn intuitively at will. The trajectories of the input query were translated from the input sequence information whereby the motion was drawn. The proposed retrieval engine was able to achieve an accuracy of 100\% for the top-3 results based on simple trajectories. However, when presented with complex trajectories that involved multiple turnings, the accuracy dropped to 67\%. Overall, the retrieval engine achieved an average accuracy of 82\% for the top-3 results.

While the proposed retrieval engines were able to successfully retrieve the desired video footages to a good extent, there are still some inherent weaknesses. In terms of increasing its usefulness, the retrieval engine can be extended to retrieve shots containing multiple trajectories. This would be very useful in scenarios involving multiple vehicles such as accidents. Next, the retrieval engine can also be extended to include specific parking spots of the vehicles. In the current setup, the retrieval engine only evaluates the results based on its overall trajectories or movements within the car park. However, in a car park scenario, the parking location of the vehicle should also be considered as an \emph{event} while describing activities in a car park.

\vspace{1em}
\subsection*{Final Remark}
This work done in this thesis has achieved the objectives of identifying and extracting useful object semantics that can accurately describe the events in a car park scenario. A retrieval engine that takes in user-described trajectories along with other object specific semantics such as colour as well as time and date information allows users to easily define the desired queries in a realistic manner. As the retrieval engine was developed using web based technology, it is easy for such systems to be implemented directly on modern web browsers which are fully independent of the underlying Operating System (OS) of the machine. This also allows the results to be retrieved quicker as the resulting videos do not need to be decoded and re-encoded to specific time periods as \emph{HTML5} video supports `Media Fragments URI'. For instance, to play a video from the 10th second to the 20th second, a time parameter (\emph{i.e.} $t=10,20$) can easily specified. While the overall goal of this thesis was achieved, we presented a number of future extensions for the betterment of the proposed system.